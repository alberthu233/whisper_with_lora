{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized Whisper Finetuning With Lora Implementation\n",
    "\n",
    "In this notebook, we will implement the Lora finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict,  Audio, load_from_disk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from transformers import WhisperFeatureExtractor, AdamW\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification, Wav2Vec2Processor, AutoTokenizer, AutoModelForCTC, Wav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2Tokenizer\n",
    "\n",
    "import whisper\n",
    "from whisper.audio import log_mel_spectrogram, pad_or_trim\n",
    "from whisper.model import Whisper\n",
    "from whisper.tokenizer import Tokenizer, get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import multiprocessing\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model('small')\n",
    "encoder = whisper_model.encoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_channel_ligo_binary_classifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.ln_post.normalized_shape[0] * 2, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_tensor_0, mel_tensor_1):\n",
    "        output_h1 = self.encoder(mel_tensor_0)[:, -1, :]\n",
    "        output_l1 = self.encoder(mel_tensor_1)[:, -1, :]\n",
    "        outputs = torch.cat((output_h1, output_l1), dim=1)\n",
    "        logits = self.classifier(outputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 89.232385M\n"
     ]
    }
   ],
   "source": [
    "model_ft = two_channel_ligo_binary_classifier(encoder)\n",
    "\n",
    "# unfreeze all parameters for full finetuning\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# count the trainable of parameters\n",
    "num_params = sum(p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
    "# print parameters in millions\n",
    "print(f\"Number of parameters: {num_params/1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_channel_ligo_binary_classifier(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2.230273M\n"
     ]
    }
   ],
   "source": [
    "model_freeze_encoder = two_channel_ligo_binary_classifier(encoder)\n",
    "\n",
    "# freeze the encoder parameters\n",
    "for param in model_freeze_encoder.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# count the trainable of parameters\n",
    "num_params = sum(p.numel() for p in model_freeze_encoder.parameters() if p.requires_grad)\n",
    "# print parameters in millions\n",
    "print(f\"Number of trainable parameters: {num_params/1e6}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding Lora from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA_layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA_layer is a custom PyTorch module that implements the LoRA (Low-Rank Adaptation) mechanism.\n",
    "\n",
    "    Args:\n",
    "        in_dim (int): The input dimension of the layer.\n",
    "        out_dim (int): The output dimension of the layer.\n",
    "        rank (int): The rank of the low-rank approximation.\n",
    "        alpha (float): The scaling factor for the output.\n",
    "\n",
    "    Attributes:\n",
    "        A (torch.nn.Parameter): The learnable parameter representing the low-rank matrix A.\n",
    "        B (torch.nn.Parameter): The learnable parameter representing the low-rank matrix B.\n",
    "        alpha (float): The scaling factor for the output.\n",
    "        rank (int): The rank of the low-rank approximation.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Performs the forward pass of the LoRA layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha / self.rank * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "class LoRa_linear(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRA_layer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_lora = two_channel_ligo_binary_classifier(encoder)\n",
    "# freeze the parameters of the model\n",
    "for param in whisper_lora.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_channel_ligo_binary_classifier(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): LoRa_linear(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRA_layer()\n",
      "          )\n",
      "          (key): LoRa_linear(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (lora): LoRA_layer()\n",
      "          )\n",
      "          (value): LoRa_linear(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRA_layer()\n",
      "          )\n",
      "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): LoRa_linear(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRA_layer()\n",
      "          )\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): LoRa_linear(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRA_layer()\n",
      "          )\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class lora_params:\n",
    "    rank = 4\n",
    "    alpha = 4\n",
    "    query = True\n",
    "    key = True\n",
    "    mlp = True\n",
    "    value = True\n",
    "    head = False\n",
    "\n",
    "# replace the linear layer with LoRA_linear\n",
    "replace_lora = partial(LoRa_linear, rank=lora_params.rank, alpha=lora_params.alpha)\n",
    "\n",
    "for layer in whisper_lora.encoder.blocks:\n",
    "    if lora_params.query:\n",
    "        layer.attn.query = replace_lora(layer.attn.query)\n",
    "    if lora_params.key:\n",
    "        layer.attn.key = replace_lora(layer.attn.key)\n",
    "    if lora_params.value:\n",
    "        layer.attn.value = replace_lora(layer.attn.value)\n",
    "    if lora_params.mlp:\n",
    "        for i, mlp_layer in enumerate(layer.mlp):\n",
    "            if isinstance(mlp_layer, torch.nn.Linear):\n",
    "                layer.mlp[i] = replace_lora(mlp_layer)\n",
    "        \n",
    "if lora_params.head:\n",
    "    for layer in whisper_lora.classifier:\n",
    "        layer = replace_lora(layer)\n",
    "\n",
    "print(whisper_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the Q K V layer in the multihead attention has been replaced by the Lora layer.\n",
    "This implementation can also be applied to any other other model with linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2.820097M\n"
     ]
    }
   ],
   "source": [
    "# count the trainable of parameters\n",
    "num_params = sum(p.numel() for p in whisper_lora.parameters() if p.requires_grad)\n",
    "# print parameters in millions\n",
    "print(f\"Number of trainable parameters: {num_params/1e6}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have three model to compare, the original **model with full parameters fine-tuning**, the original **model with encoder part freezed**, and the original **model with the encoder part freezed and applied lora linear layer**. The trainable parameters of the three models are **89M**, **2.32M** and **2.82M** respectively. We will compare the performance of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
