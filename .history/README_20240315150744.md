# LoRA: Low-Rank Adaptation of Large Language Models

## Introduction
- Adapting large language models to downstream tasks is useful but challenging
  - Fine-tuning is expensive and resource-intensive
  - Models like GPT-3 (175B parameters) require significant resources
- Existing parameter-efficient adaptation techniques have limitations
  - Adapter layers increase inference latency
  - Prompt-based methods reduce available sequence length
- LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)
  - Efficient adaptation technique
  - Adds low-rank update matrices to pre-trained weights
  - Reduces trainable parameters while maintaining performance
- Presentation Overview
  - Explore LoRA paper in detail
  - Discuss mathematical formulation and implementation
  - Present experimental results comparing LoRA with other adaptation methods, full finetuning, freeze part of model, and LoRA adaptation.


## Background
$W_0 \in \mathbb{R}^{d \times k}$

$W_0 + \Delta W = W_0 + BA$

$B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, r \ll \min(d, k)$

```{latex}
We use a random Gaussian initialization for A and zero for B, so \Delta W = BA is zero at the beginning of training. We then scale \Delta Wx by \frac{\alpha}{\sqrt{r}}, where \alpha is a constant in \mathbb{R}. When optimizing with Adam, tuning \alpha is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set \alpha to the first \sqrt{r} we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r \citep{yang2021}.
```

### Fine-Tuning and Parameter-Efficient Adaptation
- Discuss the traditional fine-tuning approach and its drawbacks for large models
- Introduce parameter-efficient adaptation techniques like adapter layers and prompt-based methods
- Mention their limitations, such as increased inference latency or reduced sequence length

## LoRA: Low-Rank Adaptation
- Explain the key idea behind LoRA: adding low-rank update matrices to the pre-trained weights
- Describe the mathematical formulation of LoRA and how it modifies the attention layers
- Highlight the advantages of LoRA, such as reduced training cost and inference latency

## Experiments and Results
### Experimental Setup
- Describe the datasets and tasks used for evaluation (e.g., sequence classification with Whisper model)
- Mention the baselines compared against LoRA (e.g., full fine-tuning, frozen encoder with trained classification head)

### Implementation Details
- Briefly explain the implementation of LoRA in the Whisper model for sequence classification
- Provide code snippets or refer to the accompanying Jupyter notebook for implementation details

### Results and Analysis
- Present the plots comparing the performance of LoRA against the baselines
  - Training time vs. Validation AUC
  - Training time vs. Validation Loss
- Analyze the results and highlight the benefits of LoRA in terms of efficiency and performance
- Discuss any observations or insights gained from the experiments

## Conclusion
- Summarize the key findings and contributions of the LoRA paper
- Emphasize the effectiveness of LoRA as an efficient adaptation technique for large language models
- Mention potential future directions or applications of LoRA

## References
- List the references cited in the presentation, including the LoRA paper and any other relevant works

## Appendix
- Include any additional details or experiments that couldn't fit in the main presentation
- Provide links to the accompanying Jupyter notebook or other resources